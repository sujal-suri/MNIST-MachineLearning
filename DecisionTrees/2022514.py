# -*- coding: utf-8 -*-
"""Number_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17UetkVcpXHxxRk9EPwVUvWRL3H8puF_o
"""

#imports.
import numpy as np
import matplotlib.pyplot as plt
import urllib
#retriving the data set from the source.
url = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'

urllib.request.urlretrieve(url, 'mnist.npz')

with np.load('mnist.npz') as data:
    train_images = data['x_train']
    train_labels = data['y_train']
    test_images = data['x_test']
    test_labels = data['y_test']

#Vectorizing the training samples and Test Samples.
Vectorized_train_images=train_images.reshape(60000,-1)
Vectorized_test_images=test_images.reshape(10000,-1)
print(Vectorized_test_images.shape)

#Dictionary for each class as key and DataMatrix, MeanVector, Covariance as values.
Vectorized_train_images_dict = {i : [] for i in range(10)}
Vectorized_test_images_dict = {i : [] for i in range(10)}
Vectorized_train_mean_dict= {}
Vectorized_train_covariance_dict = {}
Prior_probability_dict = {}

for image, label in zip(Vectorized_test_images,test_labels):
  Vectorized_test_images_dict[label].append(image)

for image, label in zip(Vectorized_train_images, train_labels):
  Vectorized_train_images_dict[label].append(image)

for label in range(10):
  Vectorized_train_mean_dict[label]=np.mean(Vectorized_train_images_dict[label], axis=0)

for label in range(10):
  data = (Vectorized_train_images_dict[label])
  Vectorized_train_covariance_dict[label]=np.cov(data,rowvar=False)

Total = 60000
for label in Vectorized_train_images_dict.keys():
  Prior_probability_dict[label]=len(Vectorized_train_images_dict[label])/Total

for label in range(10):
  for i in range(5):
    plt.subplot(1,5,i+1)
    plt.imshow(Vectorized_test_images_dict[label][i].reshape(28,28),cmap='gray')
    plt.axis('off')
  plt.show()

Const_dict = {}
Linear_dict = {}
Quadratic_dict = {}

def calc_QDA_coeff(label,mean, cov,prior):
  lamda = 0.000001
  const1 = -(0.5)*np.dot(np.dot(mean.T,np.linalg.pinv(cov)),mean)
  const2 = -(0.5)*np.log(np.linalg.det(cov)+lamda)
  const3 = np.log(prior)
  Const_dict[label]=const1+const2+const3

  lin = np.dot(np.linalg.pinv(cov),mean)
  Linear_dict[label]=lin

  quad = -(0.5)*np.linalg.pinv(cov)
  Quadratic_dict[label]=quad

for label in Vectorized_train_images_dict.keys():
  calc_QDA_coeff(label,Vectorized_train_mean_dict[label],Vectorized_train_covariance_dict[label],Prior_probability_dict[label])

def quadratic_discriminant_analysis(X, label):
    zero_mean_X = X #- Vectorized_train_mean_dict[label]
    first = np.dot(np.dot(zero_mean_X.T, Quadratic_dict[label]), zero_mean_X)
    sec_transpose = np.transpose(Linear_dict[label])
    second = np.dot(sec_transpose, zero_mean_X)
    third = Const_dict[label]
    return first + second + third

correct = 0
correct_class_dict = {i: 0 for i in range(10)}
for image,actual in zip(Vectorized_test_images,test_labels):
  #print(image.shape)
  max = quadratic_discriminant_analysis(image,0)
  number = 0
  for label in range(10):
    discriminant = quadratic_discriminant_analysis(image,label)
    #print(discriminant,"for label = ",label)
    if(discriminant>max):
      max = discriminant
      number = label
  if(number==actual):
    correct_class_dict[number]+=1
    correct+=1
print("accuracy: ",correct/10000)

for i in range(10):
  print("accuracy class of ",i," = ",correct_class_dict[i]/len(Vectorized_test_images_dict[i]))

"""Starting of Problem2"""

import numpy as np
import matplotlib.pyplot as plt
import urllib

url = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'

urllib.request.urlretrieve(url, 'mnist.npz')

with np.load('mnist.npz') as data:
    train_images = data['x_train']
    train_labels = data['y_train']
    test_images = data['x_test']
    test_labels = data['y_test']

Vectorized_train_images=train_images.reshape(60000,-1)
Vectorized_test_images=test_images.reshape(10000,-1)

Vectorized_train_subset_image = {i : [] for i in range(10)}

for image, label in zip(Vectorized_train_images,train_labels):
  if(len(Vectorized_train_subset_image[label])<100):
    Vectorized_train_subset_image[label].append(image)

dataset = np.zeros((1000,784))
fill = 0

for label in range(10):
  for i in range(len(Vectorized_train_subset_image[label])):
    dataset[fill]=(Vectorized_train_subset_image[label][i])
    fill+=1

dataset=dataset.T

mean_dataset = np.mean(dataset,axis=1,keepdims=True)
Zero_mean_dataset = dataset - mean_dataset
Zero_mean_cov = np.dot(Zero_mean_dataset,Zero_mean_dataset.T)/999

eigenvalues , eigenvectors = np.linalg.eig(Zero_mean_cov)
norms = np.linalg.norm(eigenvectors, axis=1, keepdims=True)
eigenvectors = eigenvectors/norms
sorted_indices = np.argsort(eigenvalues)[::-1]
sorted_eigenvalues = eigenvalues[sorted_indices]
sorted_eigenvectors = eigenvectors[:, sorted_indices]

Y = np.dot((sorted_eigenvectors.T),Zero_mean_dataset)
X_reconst = np.dot(sorted_eigenvectors,Y)

def mean_square_diff(matrix1, matrix2):
  sq_diff = (matrix1-matrix2)**2
  return np.mean(sq_diff)

parray = [5,10,20,500]
for p in parray:
  p_eigenvectors = sorted_eigenvectors[:, :p]
  new_Y = np.dot(p_eigenvectors.T,Zero_mean_dataset)
  pca_reconstruction = np.dot(p_eigenvectors,new_Y)
  for i in range(5):
    plt.subplot(1,5,i+1)
    plt.imshow(np.real(pca_reconstruction[:,i*100]).reshape(28,28),cmap='gray')
    plt.title('p = {}'.format(p))
    plt.axis('off')
  plt.show()

p_eig = sorted_eigenvectors[:, :10]

p_X_test = np.dot(p_eig.T,(Vectorized_test_images.T))